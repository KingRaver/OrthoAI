# ğŸ§  Hacker Reign - LLM Models Reference Guide

> Complete reference for local LLM models optimized for Python & TypeScript/Next.js development
> 
> **Hardware**: M4 MacBook Air 16GB | **Platform**: Ollama

---

## Table of Contents

- [Understanding Model Sizes (B Ratings)](#understanding-model-sizes-b-ratings)
- [MoE Models Explained](#moe-models-explained)
- [Quantization Guide](#quantization-guide)
- [RAM Requirements](#ram-requirements)
- [Recommended Models for 16GB Mac](#recommended-models-for-16gb-mac)
- [Model Download Commands](#model-download-commands)
- [Model Comparison Tables](#model-comparison-tables)
- [Configuration Examples](#configuration-examples)

---

## Understanding Model Sizes (B Ratings)

### What Does "B" Mean?

**B = Billion Parameters**

Parameters are the learned weights in a neural network - the numbers the model uses to make predictions. More parameters generally means:
- âœ… Better reasoning and code quality
- âœ… More nuanced understanding
- âŒ More RAM/VRAM required
- âŒ Slower inference speed

### Simple Analogy

Think of parameters like brain synapses:
- **7B** = 7 billion connections (smart assistant)
- **70B** = 70 billion connections (expert consultant)
- **480B** = 480 billion connections (genius-level)

### Size Categories

| Category | Parameters | Use Case | Quality |
|----------|------------|----------|---------|
| **Tiny** | 1B-3B | Quick completions, edge devices | Basic |
| **Small** | 7B-9B | Daily coding, fast responses | Good |
| **Medium** | 13B-16B | Complex tasks, better reasoning | Very Good |
| **Large** | 30B-34B | Production code, multi-file | Excellent |
| **XL** | 70B+ | Research, frontier tasks | State-of-art |

---

## MoE Models Explained

### What is MoE?

**MoE = Mixture of Experts**

Instead of using ALL parameters for every request, MoE models activate only a subset of "expert" sub-networks. This gives you big-model quality with small-model speed.

### Reading MoE Model Names

```
Qwen3-Coder-480B-A35B-Instruct
            â”‚     â”‚
            â”‚     â””â”€â”€ Active: Only 35B parameters used per token
            â””â”€â”€â”€â”€â”€â”€â”€â”€ Total: 480B parameters available

DeepSeek-Coder-V2-236B (21B active)
                  â”‚     â”‚
                  â”‚     â””â”€â”€ Only 21B active per request
                  â””â”€â”€â”€â”€â”€â”€â”€â”€ 236B total parameters
```

### Why MoE Matters for You

| Model | Total Params | Active Params | Effective RAM Need |
|-------|--------------|---------------|-------------------|
| Qwen3-Coder 30B | 30B | **3B** | ~8GB (Q4) âœ… |
| Qwen3-Coder 480B | 480B | **35B** | ~150GB âŒ |
| DeepSeek-V2 16B | 16B | **2.4B** | ~6GB (Q4) âœ… |
| DeepSeek-V3 671B | 671B | **37B** | ~200GB âŒ |

**Bottom Line**: MoE models let you run "30B quality" on 16GB hardware!

---

## Quantization Guide

### What is Quantization?

Quantization reduces the precision of model weights to shrink file size and RAM usage.

```
Full Precision (FP16): Each weight = 16 bits (2 bytes)
4-bit Quantized (Q4):  Each weight = 4 bits (0.5 bytes)
                       = 75% size reduction!
```

### Quantization Suffixes Explained

| Suffix | Bits | Quality | Size | Speed | Use Case |
|--------|------|---------|------|-------|----------|
| `fp16` / `bf16` | 16 | 100% | 1x | Slow | Research, max quality |
| `q8_0` | 8 | ~99% | 0.5x | Medium | Quality-focused |
| `q6_K` | 6 | ~97% | 0.4x | Medium | Good balance |
| `q5_K_M` | 5 | ~95% | 0.35x | Fast | **Recommended** |
| `q4_K_M` | 4 | ~90% | 0.25x | Fast | **Best for 16GB** |
| `q4_0` | 4 | ~85% | 0.25x | Fastest | Speed priority |
| `q3_K_M` | 3 | ~80% | 0.2x | Fastest | Tight RAM |
| `q2_K` | 2 | ~70% | 0.15x | Fastest | Last resort |

### Quantization Naming Convention

```
qwen2.5-coder:7b-instruct-q5_K_M
              â”‚    â”‚       â”‚  â”‚
              â”‚    â”‚       â”‚  â””â”€â”€ M = Medium (balanced quality/size)
              â”‚    â”‚       â””â”€â”€â”€â”€â”€ K = K-quant method (better quality)
              â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ instruct = Chat/instruction tuned
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7b = 7 billion parameters
```

### Quality vs Size Trade-off

```
Quality â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%  FP16 (full)
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  99%   Q8
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    97%   Q6_K
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      95%   Q5_K_M â† Sweet spot
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        90%   Q4_K_M â† Best for 16GB
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              85%   Q4_0
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    80%   Q3_K_M
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          70%   Q2_K
```

---

## RAM Requirements

### Formula

```
Full Precision (FP16):
  RAM = Parameters (B) Ã— 2 GB
  Example: 7B Ã— 2 = 14GB

Quantized (Q4):
  RAM = Parameters (B) Ã— 0.5 GB + 2GB overhead
  Example: 7B Ã— 0.5 + 2 = 5.5GB
```

### Quick Reference Table

| Model Size | FP16 RAM | Q8 RAM | Q5 RAM | Q4 RAM | Fits 16GB? |
|------------|----------|--------|--------|--------|------------|
| **3B** | 6GB | 4GB | 3GB | 2.5GB | âœ… Any |
| **7B** | 14GB | 9GB | 6GB | 5GB | âœ… Q5 or lower |
| **13B** | 26GB | 15GB | 10GB | 8GB | âœ… Q4 only |
| **16B** | 32GB | 18GB | 12GB | 10GB | âš ï¸ Q4, tight |
| **30B** | 60GB | 35GB | 22GB | 17GB | âš ï¸ Q4 MoE only |
| **34B** | 68GB | 38GB | 25GB | 19GB | âŒ Too large |
| **70B** | 140GB | 75GB | 50GB | 38GB | âŒ Too large |

### Your M4 16GB Sweet Spots

```
âœ… Perfect Fit:    3B-7B at Q5_K_M or Q4_K_M
âœ… Good Fit:       13B at Q4_K_M
âš ï¸ Tight Fit:      16B at Q4_0, may swap
âš ï¸ MoE Exception:  30B-A3B (only 3B active) at Q4
âŒ Won't Fit:      34B+ dense models
```

---

## Recommended Models for 16GB Mac

### Tier 1: Daily Drivers (Fast & Reliable)

| Model | Why | Speed | Quality |
|-------|-----|-------|---------|
| **Yi-Coder 9B** | Built for web dev (Python, JS, TS, Node) | ğŸš€ğŸš€ğŸš€ | â­â­â­â­ |
| **Qwen2.5-Coder 7B** | Your current model, excellent all-rounder | ğŸš€ğŸš€ğŸš€ | â­â­â­â­ |
| **DeepSeek-Coder-V2 16B** | GPT-4 level coding, MoE efficient | ğŸš€ğŸš€ | â­â­â­â­â­ |
| **CodeGemma 7B** | Google's fast, lightweight coder | ğŸš€ğŸš€ğŸš€ğŸš€ | â­â­â­ |

### Tier 2: Power Models (Complex Tasks)

| Model | Why | Speed | Quality |
|-------|-----|-------|---------|
| **Qwen3-Coder 30B-A3B** | 30B quality, only 3B active (MoE magic) | ğŸš€ğŸš€ | â­â­â­â­â­ |
| **Codestral 22B** | Mistral's code specialist, 32K context | ğŸš€ | â­â­â­â­â­ |
| **CodeLlama 13B** | Meta's proven coder, good for Python | ğŸš€ğŸš€ | â­â­â­â­ |

### Tier 3: Specialists

| Model | Specialty | Best For |
|-------|-----------|----------|
| **StarCoder2 15B** | Multi-language, permissive license | Fine-tuning, open source projects |
| **Llama 3.2 3B** | Ultra-fast, general purpose | Quick chats, simple completions |
| **Mistral 7B** | Strong reasoning | Explaining code, documentation |

---

## Model Download Commands

### ğŸš€ Quick Start - Essential Models

```bash
# Your current setup (already have)
ollama pull qwen2.5-coder:7b-instruct-q5_K_M
ollama pull llama3.2:3b-instruct-q5_K_M

# Recommended additions
ollama pull yi-coder:9b                      # Best for web dev
ollama pull deepseek-coder-v2:16b            # GPT-4 level coding
```

### ğŸ“¦ Full Model Library

#### Coding Specialists

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# QWEN FAMILY (Alibaba) - Excellent for Python & TypeScript
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Qwen 2.5 Coder - Your current model, proven performer
ollama pull qwen2.5-coder:7b-instruct-q5_K_M     # 5GB - Daily driver
ollama pull qwen2.5-coder:7b-instruct-q4_K_M     # 4GB - Faster, slightly less quality
ollama pull qwen2.5-coder:14b-instruct-q4_K_M    # 9GB - Better reasoning
ollama pull qwen2.5-coder:32b-instruct-q4_K_M    # 20GB - Won't fit 16GB

# Qwen 3 Coder - Newest, agentic capabilities (MoE)
ollama pull qwen3-coder:30b                       # ~18GB - MoE, only 3B active!

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEEPSEEK FAMILY - GPT-4 Level Performance
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# DeepSeek Coder V2 - Rivals GPT-4 Turbo for coding
ollama pull deepseek-coder-v2:16b                # 10GB - Recommended
ollama pull deepseek-coder-v2:16b-lite-instruct-q4_K_M  # 6GB - Lighter

# DeepSeek Coder V1 - Still excellent
ollama pull deepseek-coder:6.7b                  # 4GB - Fast
ollama pull deepseek-coder:33b-instruct-q4_K_M   # 20GB - Won't fit

# DeepSeek V3 - Frontier model (needs 200GB+ RAM)
# ollama pull deepseek-v3                        # Too large for local

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# YI-CODER - Optimized for Web Development
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Best for Python, JavaScript, TypeScript, Node, HTML, SQL
ollama pull yi-coder:9b                          # 6GB - Highly recommended!
ollama pull yi-coder:9b-chat                     # 6GB - Chat-tuned variant
ollama pull yi-coder:1.5b                        # 1GB - Ultra-light

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CODELLAMA (Meta) - Battle-tested, Wide Language Support
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ollama pull codellama:7b-instruct                # 4GB - Fast
ollama pull codellama:13b-instruct               # 8GB - Better quality
ollama pull codellama:34b-instruct-q4_0          # 20GB - Won't fit
ollama pull codellama:7b-python                  # 4GB - Python specialist

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CODESTRAL (Mistral) - Strong Reasoning, 32K Context
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ollama pull codestral:22b                        # 14GB - Tight fit
ollama pull codestral:22b-v0.1-q4_K_M            # 12GB - Quantized

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CODEGEMMA (Google) - Lightweight, Fast
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ollama pull codegemma:7b                         # 5GB - Good all-rounder
ollama pull codegemma:7b-instruct                # 5GB - Chat-tuned
ollama pull codegemma:2b                         # 1.5GB - Ultra-fast

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STARCODER2 - Permissive License, Great for Fine-tuning
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ollama pull starcoder2:15b                       # 10GB - Full featured
ollama pull starcoder2:7b                        # 5GB - Balanced
ollama pull starcoder2:3b                        # 2GB - Lightweight
```

#### General Purpose (Also Good for Coding)

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LLAMA 3.x (Meta) - Excellent General + Coding
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ollama pull llama3.2:3b-instruct-q5_K_M          # 2GB - Fast general
ollama pull llama3.2:3b                          # 2GB - Base model
ollama pull llama3.1:8b-instruct-q5_K_M          # 5GB - Better reasoning

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MISTRAL - Strong Reasoning
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ollama pull mistral:7b-instruct                  # 4GB - Great reasoning
ollama pull mistral-nemo:12b                     # 8GB - Newer, better

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# QWEN 2.5 General - Non-coder variants
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ollama pull qwen2.5:7b-instruct-q5_K_M           # 5GB - General purpose
ollama pull qwen2.5:14b-instruct-q4_K_M          # 9GB - Better reasoning
```

#### Embedding Models (For RAG)

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EMBEDDING MODELS - For your memory/RAG system
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ollama pull nomic-embed-text                     # 137MB - Recommended
ollama pull mxbai-embed-large                    # 670MB - Higher accuracy
ollama pull all-minilm                           # 45MB - Ultra-light
```

### ğŸ§¹ Model Management Commands

```bash
# List all downloaded models
ollama list

# Show model details
ollama show qwen2.5-coder:7b-instruct-q5_K_M

# Remove a model
ollama rm codellama:7b-instruct

# Check disk usage
du -sh ~/.ollama/models

# Update a model
ollama pull qwen2.5-coder:7b-instruct-q5_K_M

# Run model interactively (test)
ollama run yi-coder:9b "Write a Python function to reverse a string"

# Keep model loaded in memory (faster responses)
OLLAMA_KEEP_ALIVE=-1 ollama serve
```

---

## Model Comparison Tables

### Coding Benchmarks (HumanEval)

Higher is better. HumanEval tests ability to write correct Python functions.

| Model | HumanEval | Pass@1 | Notes |
|-------|-----------|--------|-------|
| GPT-4 Turbo | 87% | - | Closed source baseline |
| Claude 3.5 Sonnet | 85% | - | Closed source baseline |
| **DeepSeek-Coder-V2 236B** | 81% | 90% | Best open source |
| **Qwen2.5-Coder 32B** | 79% | 87% | Excellent |
| **Qwen2.5-Coder 7B** | 68% | - | Your current model |
| **Yi-Coder 9B** | 65% | - | Web dev optimized |
| **CodeLlama 34B** | 62% | 77% | Proven performer |
| **DeepSeek-Coder-V2 16B** | 60% | - | Great for size |
| **StarCoder2 15B** | 58% | - | Permissive license |
| **CodeGemma 7B** | 52% | - | Fast & lightweight |

### Speed vs Quality (Your Hardware)

| Model | Tokens/sec* | Quality | RAM Used |
|-------|-------------|---------|----------|
| Llama 3.2 3B | ~80 t/s | â­â­ | 3GB |
| CodeGemma 7B | ~45 t/s | â­â­â­ | 5GB |
| Yi-Coder 9B | ~35 t/s | â­â­â­â­ | 6GB |
| Qwen2.5-Coder 7B (Q5) | ~40 t/s | â­â­â­â­ | 5GB |
| DeepSeek-Coder-V2 16B | ~25 t/s | â­â­â­â­â­ | 10GB |
| Qwen3-Coder 30B (MoE) | ~20 t/s | â­â­â­â­â­ | 12GB |
| Codestral 22B (Q4) | ~15 t/s | â­â­â­â­â­ | 14GB |

*Approximate, varies by prompt length and system load

### Language Support Comparison

| Model | Python | TypeScript | JavaScript | React/Next.js | SQL |
|-------|--------|------------|------------|---------------|-----|
| **Yi-Coder 9B** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| **Qwen2.5-Coder 7B** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |
| **DeepSeek-Coder-V2** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| **CodeLlama 13B** | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­ |
| **Codestral 22B** | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |
| **StarCoder2 15B** | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­ |

---

## Configuration Examples

### Chat.tsx Model Selector

```typescript
// Recommended model configuration for Hacker Reign
const models = [
  // Fast - Daily coding
  { id: 'llama3.2:3b-instruct-q5_K_M', name: 'Llama 3.2', speed: 'ğŸš€' },
  
  // Balanced - Your current go-to
  { id: 'qwen2.5-coder:7b-instruct-q5_K_M', name: 'Vibe Coder', speed: 'âš¡' },
  
  // Web Dev Specialist
  { id: 'yi-coder:9b', name: 'Yi-Coder', speed: 'ğŸŒ' },
  
  // Power - Complex tasks
  { id: 'deepseek-coder-v2:16b', name: 'DeepSeek V2', speed: 'ğŸ§ ' },
  
  // Agentic - Multi-file projects
  { id: 'qwen3-coder:30b', name: 'Qwen3 Agent', speed: 'ğŸ¯' },
];
```

### Multi-Model Strategy

```typescript
// Strategy: Use different models for different tasks
const modelStrategy = {
  // Quick completions, simple questions
  quick: 'llama3.2:3b-instruct-q5_K_M',
  
  // Daily Python/TypeScript coding
  coding: 'qwen2.5-coder:7b-instruct-q5_K_M',
  
  // Full-stack web development
  webdev: 'yi-coder:9b',
  
  // Complex debugging, architecture decisions
  complex: 'deepseek-coder-v2:16b',
  
  // Multi-file refactoring, large codebases
  agentic: 'qwen3-coder:30b',
  
  // Embeddings for RAG/memory
  embedding: 'nomic-embed-text',
};
```

### Ollama Modelfile (Custom Configuration)

```dockerfile
# Save as: ~/.ollama/Modelfile.hackerreign
FROM qwen2.5-coder:7b-instruct-q5_K_M

# Optimized for coding tasks
PARAMETER temperature 0.3
PARAMETER top_p 0.85
PARAMETER top_k 40
PARAMETER repeat_penalty 1.2
PARAMETER num_ctx 16384
PARAMETER num_predict 5555

SYSTEM """You are Hacker Reign - a friendly coding expert specializing in Python and TypeScript/Next.js.

RULES:
- Respond in plain text, no markdown
- Be concise (1-3 sentences for simple questions)
- Write production-ready code
- Explain your reasoning briefly
"""
```

Create custom model:
```bash
ollama create hackerreign -f ~/.ollama/Modelfile.hackerreign
ollama run hackerreign
```

---

## Troubleshooting

### Model Won't Load

```bash
# Check available memory
vm_stat | grep "Pages free"

# Close other apps, then try
ollama run yi-coder:9b

# If still failing, try smaller quantization
ollama pull qwen2.5-coder:7b-instruct-q4_K_M
```

### Slow Generation

```bash
# Keep model loaded
OLLAMA_KEEP_ALIVE=-1 ollama serve

# Or set per-session
export OLLAMA_KEEP_ALIVE=-1

# Check if GPU is being used
ollama ps
```

### Out of Memory

```bash
# Use smaller model
ollama pull yi-coder:1.5b

# Or use more aggressive quantization
ollama pull qwen2.5-coder:7b-instruct-q4_0

# Clear cached models
ollama rm unused-model-name
```

---

## Quick Reference Card

### Best Models for Your Stack

| Task | Model | Command |
|------|-------|---------|
| **Python + TypeScript daily** | Yi-Coder 9B | `ollama pull yi-coder:9b` |
| **Quick responses** | Llama 3.2 3B | `ollama pull llama3.2:3b` |
| **Complex debugging** | DeepSeek-Coder-V2 | `ollama pull deepseek-coder-v2:16b` |
| **Multi-file refactor** | Qwen3-Coder 30B | `ollama pull qwen3-coder:30b` |
| **RAG embeddings** | Nomic Embed | `ollama pull nomic-embed-text` |

### Size Quick Guide

| RAM Available | Max Model Size | Best Option |
|---------------|----------------|-------------|
| 8GB | 7B Q4 | `codegamma:7b` |
| 16GB | 13B Q4 or 30B MoE | `yi-coder:9b` |
| 32GB | 30B Q5 | `codestral:22b` |
| 64GB+ | 70B Q4 | `llama3.1:70b` |

---

## Changelog

- **Jan 2026**: Initial version with M4 16GB optimizations
- Models tested: Qwen2.5-Coder, Yi-Coder, DeepSeek-Coder-V2, Qwen3-Coder

---

*Last Updated: January 9, 2026*
*Hardware: M4 MacBook Air 16GB*
*Ollama Version: Latest*